{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary packages\n",
        "\n",
        "##1-YOLO from Ultralytics\n",
        "##2- Deep SORT for tracking"
      ],
      "metadata": {
        "id": "wslAOTO-zDgk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "idl_9RdUN1uJ",
        "outputId": "627d5a49-3545-4fe4-db3c-6fbadde9c823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q ultralytics\n",
        "!pip install -q deep_sort_realtime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries\n"
      ],
      "metadata": {
        "id": "yCGxBtg20GMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=r\".*torch\\.cuda\\.amp\\.autocast.*deprecated.*\",\n",
        "    category=FutureWarning\n",
        ")\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import math\n"
      ],
      "metadata": {
        "id": "odhriEwPrNiE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define input and output video paths\n"
      ],
      "metadata": {
        "id": "THhuNLhL0XJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Paths to input and output videos\n",
        "input_video_path = '/content/input.mp4'\n",
        "output_video_path = 'output_video.mp4'"
      ],
      "metadata": {
        "id": "zH3_pommrPCb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set detection and tracking parameters"
      ],
      "metadata": {
        "id": "Fd5nnz2q0dgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confidence_threshold = 0.3\n",
        "max_people = 3\n",
        "distance_threshold = 50  # max distance to consider same person\n",
        "\n",
        "# colors\n",
        "available_colors = [\n",
        "    (255, 0, 0),     # Blue\n",
        "    (0, 255, 0),     # Green\n",
        "    (0, 0, 255),     # Red\n",
        "]\n"
      ],
      "metadata": {
        "id": "LxJSR7DHv6pH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize YOLOv8 model"
      ],
      "metadata": {
        "id": "cWoTl-_c0ppT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8x.pt')\n",
        "model.fuse()\n",
        "model.conf = confidence_threshold\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwVWwtQDv_fi",
        "outputId": "c0120716-4e3a-403f-a711-e1af9e3dad88"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv8x summary (fused): 112 layers, 68,200,608 parameters, 0 gradients, 257.8 GFLOPs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open the input video and prepare the output video writer"
      ],
      "metadata": {
        "id": "ilxh9jhQ0yfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open input video\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "if not cap.isOpened():\n",
        "    raise IOError(f\"Couldn't open video file: {input_video_path}\")\n",
        "# get frame rate of input video\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "# get width and height of video frames\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))"
      ],
      "metadata": {
        "id": "Ewus8KShwBPA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare lists for tracking people and assigning IDs"
      ],
      "metadata": {
        "id": "9_8uGiya05T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assigned_ids = {}\n",
        "paths = {}\n",
        "yolo_to_custom_id = {}"
      ],
      "metadata": {
        "id": "dokN62rP1QDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define helper functions for distance calculation and ID matching"
      ],
      "metadata": {
        "id": "duo-fKbc1NYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function\n",
        "\n",
        "def euclidean_distance(p1, p2):\n",
        "    return math.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
        "\n",
        "# find the nearest available stable ID to the current center point (cx, cy)\n",
        "# used to maintain consistent tracking IDs across frames\n",
        "def get_nearest_assigned_id(cx, cy):\n",
        "    min_dist = float('inf')\n",
        "    best_id = None\n",
        "    # iterate on currently assigned IDs and their positions\n",
        "    for aid, pos in assigned_ids.items():\n",
        "        dist = euclidean_distance((cx, cy), pos)\n",
        "        # check if distance within threshold and ID is not already taken in this frame\n",
        "        if dist < distance_threshold and aid not in yolo_to_custom_id.values():\n",
        "          # if this the closest match so update best_id\n",
        "            if dist < min_dist:\n",
        "                min_dist = dist\n",
        "                best_id = aid\n",
        "    return best_id"
      ],
      "metadata": {
        "id": "7OeCxdnRwJ2K"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# video processing and tracking"
      ],
      "metadata": {
        "id": "-94UQ4c81XCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    # perform tracking on the current frame\n",
        "    results = model.track(source=frame, persist=True, verbose=False)[0]\n",
        "\n",
        "    frame_seen_ids = set()  # track which stable IDs visible in this frame\n",
        "\n",
        "    if results and results.boxes is not None:\n",
        "        for box in results.boxes:\n",
        "            class_id = int(box.cls[0])\n",
        "            if class_id != 0:\n",
        "                continue  # only persons\n",
        "\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            # get YOLO's internal track ID\n",
        "            track_id = int(box.id[0]) if box.id is not None else -1\n",
        "            if track_id == -1:\n",
        "                continue\n",
        "            # Compute bottom center point of bounding box\n",
        "            cx, cy = (x1 + x2) // 2, y2\n",
        "\n",
        "            # If this YOLO track id already mapped, use it\n",
        "            if track_id in yolo_to_custom_id:\n",
        "                stable_id = yolo_to_custom_id[track_id]\n",
        "                assigned_ids[stable_id] = (cx, cy)  # update position\n",
        "            else:\n",
        "                # try to match to previous stable IDs\n",
        "                stable_id = get_nearest_assigned_id(cx, cy)\n",
        "\n",
        "                if stable_id is not None:\n",
        "                    yolo_to_custom_id[track_id] = stable_id\n",
        "                    assigned_ids[stable_id] = (cx, cy)\n",
        "                elif len(assigned_ids) < max_people:\n",
        "                    new_id = len(assigned_ids)\n",
        "                    assigned_ids[new_id] = (cx, cy)\n",
        "                    yolo_to_custom_id[track_id] = new_id\n",
        "                    stable_id = new_id\n",
        "                else:\n",
        "                    continue\n",
        "            # mark this stable ID as seen in this frame\n",
        "            frame_seen_ids.add(stable_id)\n",
        "            # select color based on stable ID\n",
        "            color = available_colors[stable_id]\n",
        "\n",
        "            paths.setdefault(stable_id, []).append((cx, cy))\n",
        "            # draw bounding box and label on frame\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "            cv2.putText(frame, f'Person {stable_id}', (x1, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "    # remove stale YOLO track IDs disappeared in the current frame\n",
        "    current_track_ids = [int(box.id[0]) for box in results.boxes] if results.boxes is not None else []\n",
        "    yolo_to_custom_id = {tid: sid for tid, sid in yolo_to_custom_id.items() if tid in current_track_ids}\n",
        "\n",
        "    # draw motion paths for each tracked person\n",
        "    for pid, pts in paths.items():\n",
        "        if pid not in assigned_ids:\n",
        "            continue\n",
        "        color = available_colors[pid]\n",
        "        for i in range(1, len(pts)):\n",
        "            cv2.line(frame, pts[i - 1], pts[i], color, 3)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"Tracked video saved to '{output_video_path}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGfZY3WPv2Kz",
        "outputId": "638c9ae1-7df0-4f1a-b69e-459728a129db"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracked video saved to 'output_video.mp4'\n"
          ]
        }
      ]
    }
  ]
}